{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "                                                          #----------------------------------------\n",
        "                                                                   #Student Information#\n",
        "                                                          #----------------------------------------\n",
        "                                                           # David Engstrom - Student ID: 301537614\n",
        "                                                           # Brian Salas    - Student ID: 301789398\n",
        "\n",
        "#Import Pytorch\n",
        "import torch\n",
        "#Import torch neural network\n",
        "import torch.nn as nn\n",
        "#Import torch optimization\n",
        "import torch.optim as optim\n",
        "#Get the torch DataLoader class\n",
        "from torch.utils.data import DataLoader\n",
        "#Get the ready-to-use datasets and image preprocessing functions\n",
        "from torchvision import datasets, transforms\n",
        "#Handles file paths cleaner and safer across OS\n",
        "from pathlib import Path\n",
        "\n",
        "#----------------------------\n",
        "# Config / Hyperparameters\n",
        "#----------------------------\n",
        "#Number of images processed per batch (mini-batch)\n",
        "BATCH_SIZE = 128\n",
        "#Number of full passes through the dataset\n",
        "EPOCHS = 5\n",
        "#update weights in steps of 0.001 × gradient each time\n",
        "LR = 1e-3\n",
        "#Random seed for reproducibility (controls shuffling, weight init, etc.)\n",
        "SEED = 11\n",
        "#Force training to run on CPU only (per my computers request lol)\n",
        "DEVICE = \"cpu\"\n",
        "\n",
        "#Set seed for PyTorch random number generator (reproducibility)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "#----------------------------\n",
        "# MNIST is grayscale 28x28; normalize to mean=0.1307, std=0.3081 (standard for MNIST)\n",
        "#----------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "#Load the MNIST training dataset (60,000 images), downloading if not already present\n",
        "train_ds = datasets.MNIST(root=\"data\", train=True, download=True, transform=transform)\n",
        "\n",
        "#Load the MNIST test dataset (10,000 images) with the same preprocessing\n",
        "test_ds  = datasets.MNIST(root=\"data\", train=False, download=True, transform=transform)\n",
        "\n",
        "#Wrap training dataset in DataLoader: batches of BATCH_SIZE, shuffled each epoch\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2) #use 2 subprocesses to load data in parallel\n",
        "\n",
        "#Wrap test dataset in DataLoader: batches of BATCH_SIZE, no shuffling (order doesn’t matter for evaluation)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "#----------------------------\n",
        "# Define a Convolutional Neural Network (CNN) for MNIST\n",
        "#----------------------------\n",
        "class MNISTCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Convolutional feature extractor\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),   # conv layer: 1 input channel → 32 filters, keeps 28x28\n",
        "            nn.ReLU(inplace=True),                        # non-linear activation\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # conv layer: 32 → 64 filters, still 28x28\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),                              # downsample: 28x28 → 14x14\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1), # conv layer: 64 → 128 filters, keeps 14x14\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),                              # downsample: 14x14 → 7x7\n",
        "        )\n",
        "\n",
        "        # Fully connected classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),                                 # flatten feature maps into a vector\n",
        "            nn.Linear(128 * 7 * 7, 256),                  # fully connected layer\n",
        "            nn.ReLU(inplace=True),                        # activation\n",
        "            nn.Dropout(0.3),                              # dropout (30%) to reduce overfitting\n",
        "            nn.Linear(256, 10)                            # output layer: 10 classes (digits 0–9)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)      # apply convolution + pooling layers\n",
        "        x = self.classifier(x)    # apply fully connected layers\n",
        "        return x                  # return raw class scores (logits)\n",
        "\n",
        "# Initialize the model and move it to the selected device (CPU)\n",
        "model = MNISTCNN().to(DEVICE)\n",
        "\n",
        "#----------------------------\n",
        "# Loss & Optimizer\n",
        "#----------------------------\n",
        "\n",
        "# CrossEntropyLoss: combines LogSoftmax + NLLLoss (Negative Log-Likelihood Loss)\n",
        "#   - good choice for multi-class classification (digits 0–9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Adam optimizer: adaptive learning rate method\n",
        "#   - updates model parameters based on computed gradients\n",
        "#   - uses the learning rate (LR) defined earlier\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "# ---------------------------\n",
        "# Train loop for one epoch\n",
        "# ---------------------------\n",
        "def train_one_epoch(epoch):\n",
        "    model.train()                            # set model to training mode\n",
        "    running_loss, correct, total = 0.0, 0, 0 # track cumulative loss and accuracy\n",
        "\n",
        "    # iterate over training batches\n",
        "    for images, labels in train_loader:\n",
        "        # move batch to device\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()                # reset gradients from previous step\n",
        "        logits = model(images)               # forward pass: model predictions\n",
        "        loss = criterion(logits, labels)     # compute loss between predictions & true labels\n",
        "        loss.backward()                      # backpropagate to compute gradients\n",
        "        optimizer.step()                     # update weights based on gradients\n",
        "\n",
        "        # accumulate stats for reporting\n",
        "        running_loss += loss.item() * images.size(0)   # sum of batch losses\n",
        "        preds = logits.argmax(dim=1)                   # predicted class per sample\n",
        "        correct += (preds == labels).sum().item()      # count correct predictions\n",
        "        total += labels.size(0)                        # count total samples\n",
        "\n",
        "    # compute average loss and accuracy across all batches\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = correct / total\n",
        "    print(f\"Epoch {epoch}: train loss={epoch_loss:.4f}, acc={epoch_acc:.4f}\")\n",
        "\n",
        "@torch.no_grad()                   # disable gradient tracking (faster & saves memory during eval)\n",
        "def evaluate():\n",
        "    model.eval()                   # set model to evaluation mode (turns off dropout, etc.)\n",
        "    correct, total = 0, 0          # counters for accuracy\n",
        "\n",
        "    # loop through the test dataset in batches\n",
        "    for images, labels in test_loader:\n",
        "        # move data to device\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        logits = model(images)       # forward pass (no grad since @torch.no_grad)\n",
        "        preds = logits.argmax(dim=1) # get class with highest score for each image\n",
        "\n",
        "        # update counters\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total   += labels.size(0)\n",
        "\n",
        "    # return accuracy over the entire test set\n",
        "    return correct / total\n",
        "\n",
        "# ---------------------------\n",
        "# Run training loop\n",
        "# ---------------------------\n",
        "\n",
        "best_acc = 0.0                                     # track the best test accuracy seen so far\n",
        "Path(\"checkpoints\").mkdir(exist_ok=True)           # create folder to save model checkpoints if it doesn't exist\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):                 # loop over all epochs\n",
        "    train_one_epoch(epoch)                         # train on the training set\n",
        "    test_acc = evaluate()                          # evaluate on the test set\n",
        "    print(f\"Test acc after epoch {epoch}: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
        "\n",
        "    # if this is the best test accuracy so far, save the model weights\n",
        "    if test_acc > best_acc:\n",
        "        best_acc = test_acc\n",
        "        torch.save(model.state_dict(), \"checkpoints/mnist_cnn_best.pt\")\n",
        "        print(f\"Saved new best model with acc={best_acc:.4f} ({best_acc*100:.2f}%)\")\n",
        "\n",
        "# after all epochs, report the best accuracy achieved\n",
        "print(f\"Best test accuracy: {best_acc:.4f} ({best_acc*100:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ww4dhaCptlKR",
        "outputId": "94065108-2276-41d3-bde3-b49cd7d5fb32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: train loss=0.1488, acc=0.9537\n",
            "Test acc after epoch 1: 0.9892 (98.92%)\n",
            "Saved new best model with acc=0.9892 (98.92%)\n",
            "Epoch 2: train loss=0.0428, acc=0.9862\n",
            "Test acc after epoch 2: 0.9915 (99.15%)\n",
            "Saved new best model with acc=0.9915 (99.15%)\n",
            "Epoch 3: train loss=0.0289, acc=0.9911\n",
            "Test acc after epoch 3: 0.9910 (99.10%)\n",
            "Epoch 4: train loss=0.0242, acc=0.9926\n",
            "Test acc after epoch 4: 0.9928 (99.28%)\n",
            "Saved new best model with acc=0.9928 (99.28%)\n",
            "Epoch 5: train loss=0.0176, acc=0.9945\n",
            "Test acc after epoch 5: 0.9923 (99.23%)\n",
            "Best test accuracy: 0.9928 (99.28%)\n"
          ]
        }
      ]
    }
  ]
}